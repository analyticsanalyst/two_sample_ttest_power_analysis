---
title: "Two Sample T-test Power Analysis"
output:
  rmarkdown::github_document: default
  github_notebook: default
---

```{r echo=FALSE, include = FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, dpi=700, fig.cap=TRUE)
```

### Practical application
- **Setup**: we are tasked with running an online A/B test with a primary metric of revenue per visitor (e.g. total revenue during test window / unique visitors during test window). Our test will have a control group (status quo experience) and a treatment group (new experience aimed at increasing revenue per visitor). We'll use a Welch two independent sample t-test with unequal variances to assess if there's a difference in means between the control and treatment group.
- **Null hypothesis**: control mean - treatment mean = 0 (e.g. no difference in means between groups)
- **Alternative hypothesis**: control mean - treatment mean != 0 (e.g. difference in means between groups)
- **Estimating test duration with power analysis**: we can use power analysis to derive a target sample size per test group. For a given traffic allocation, the target sample size per group sets expectations on test duration and establishes test stopping criteria.

### Recap of factors that influence test duration
- **Minimum detectable effect**: The MDE represents X. holding other factors constant, there is an inverse relationship between minimum detectable effect size and sample size. In other words, smaller the effect size of interest the larger the sample size required to detect. 
- **Significance level**: probability of observing an effect by chance
- **Power**: probability of detecting an effect when one exists. A test is underpowered when there <>. A test is overpowered when <>.
- **Time to collect target sample size by group**: varies based on test traffic allocation and experiment surface area popularity. For example, an experiment on Google's home page can hit a sample size target very fast vs a small B2B startup home page which isn't getting much taffic.

### Gaining intuition for power




```{r}
n <- 50
c_mu <- 50
c_s <- 12
t_mu <- 45
t_s <- 7

welch_ttest_sim <- function(group_sample_size, 
                            control_pop_mean,
                            control_pop_sd,
                            treatment_pop_mean,
                            treatment_pop_sd) {
  ### generate random sample for control with pop mean and sd inputs
  control <- rnorm(group_sample_size, 
                   mean = control_pop_mean, 
                   sd = control_pop_sd)
  ### generate random sample for treatment with pop mean and sd inputs
  treatment <- rnorm(group_sample_size, 
                     mean = treatment_pop_mean, 
                     sd = treatment_pop_sd)
  t.test(control, treatment)$p.value
}

### 
sim_results <- replicate(10000, welch_ttest_sim(n, c_mu, c_s, t_mu, t_s)) 

### empirical power: proportion of tests that return a pvalue below 0.05
sum(sim_results < 0.05)/length(sim_results)
```
### Prove simulation ties with power function

```{r}
### pooled sd simple formula: sqrt((s1^2 _ s2^2)/2)
### ties with Cohens formula: sqrt(((n1-1)s1^2 +  (n2-1)s2^2) / (n1+n2-2))
pooled_sd <- sqrt((c_s^2 + t_s^2)/2)

diff_between_means <- c_mu-t_mu
  
effect_size <- diff_between_means/pooled_sd

pwr.t.test(n = 50, d = effect_size, sig.level = 0.05,
           type="two.sample",alternative="two.sided") 
```



### Formula for target sample size
- 


```{r}
library(tidyverse)
library(pwr)
```
### Using pwr package vs power... base functions?
- returns same result

```{r}
?power.t.test

pwr.t.test(n = NULL, 
           d = 0.05/0.25, 
           sig.level = 0.05, 
           power = 0.8, 
           type = 'two.sample',
           alternative = "two.sided")

power.t.test(delta = 0.05, sd = 0.25, 
                   sig.level = 0.05, power = 0.8, 
                   alternative = "two.sided")
```

pwr package vs base R

using cohens D

```{r}
cyl_4 <- mpg %>% filter(cyl==4)
cyl_6 <- mpg %>% filter(cyl==6)


t.test(cyl_4$hwy, cyl_6$hwy)
```


```{r}
ttest_target_sample_size_fun <- function(delta_var, sd_var) {
      res <- power.t.test(delta = delta_var, sd = sd_var, 
                   sig.level = 0.05, power = 0.8, 
                   type = "two.sample")$n
      ceiling(res)
}

ttest_target_sample_size_fun(delta_var = 0.75, sd_var = 2.25)

expand_grid(delta_input = seq(1, 5, 0.5),
            sd_input = seq(0.25, 5, 0.25)) %>%
      rowwise() %>%
      mutate(samples_per_group = ttest_target_sample_size_fun(
                  delta_var =delta_input, sd_var = sd_input
            )
      ) %>%
      ggplot(aes(x=sd_input,
                 y=samples_per_group,
                 group=factor(delta_input),
                 color=factor(delta_input))) +
      geom_point() +
      geom_line()
```

### Related topics / further research
- Variance reduction techniques which can help speed up A/B tests and/or increase statistical power for a test
-



