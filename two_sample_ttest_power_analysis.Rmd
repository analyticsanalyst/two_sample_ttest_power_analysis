---
title: "Two Sample T-test Power Analysis"
output:
  rmarkdown::github_document: default
  github_notebook: default
---

```{r echo=FALSE, include = FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, dpi=700, fig.cap=TRUE)
```

### Practical application
- **Setup**: we are tasked with running an online A/B test with a primary metric of revenue per visitor (e.g. total revenue during test window / unique visitors during test window). Our test will have a control group (status quo experience) and a treatment group (new experience aimed at increasing revenue per visitor). We'll use a Welch two independent sample t-test with unequal variances to assess if there's a difference in means between the control and treatment group means.
- **Null hypothesis**: control mean - treatment mean = 0 (e.g. no difference in means between groups)
- **Alternative hypothesis**: control mean - treatment mean != 0 (e.g. difference in means between groups)
- **Estimating test duration with power analysis**: we can use power analysis to derive a target sample size per test group. For a given traffic allocation, the target sample size per group sets expectations on test duration and establishes test stopping criteria. An experimenter users can leverage historical test surface visitor data to derive how the target sample size will take to collect.

### Recap of factors that influence test duration
- **Minimum effect size**: in our scenerio, the minimum effect size represents smallest difference that is meaningful for the business. Holding other factors constant, smaller MDEs require larger sample size. 
- **Significance level**: probability of observing an effect by chance. In other words, probability of rejecting the null hypothesis when the null hypothesis is true (i.e. claiming there is an effect when there isn't). Holding other factors constant, lowering significance level (also called alpha) increasing sample size requirements. Side bar: when our test pvalue is below the pre-specified significance level we reject the null hypothesis in favor of the alternative hypothesis (analysis only at test end when the sample size target is reached).
- **Power**: probability of detecting an effect when one exists. We call a test underpowered when the sample size is not sufficient to detect an effect size of interest. A test is overpowered when we greatly surpass the target sample size per group.
- **Time to collect target sample size by group**: varies based on test traffic allocation and experiment surface area popularity. 

### Gaining intuition for power
- We can simulate 10k iterations of an A/B test below.
- In the simulation scenario below the true effect between the groups is known.
- However, we will pretend we don't know the true effect size and use sample data to estimate the true effect.
- For the 10k simulated A/B tests, empirical power represents the proportion of pvalues below our significance level of 0.05. 
- TODO: NEEDS further research to make sure I'm tracking.

```{r}
### fake revenue per visitor metrics
n <- 50
c_mu <- 50
c_s <- 12
t_mu <- 45
t_s <- 7

welch_ttest_sim <- function(group_sample_size, 
                            control_pop_mean,
                            control_pop_sd,
                            treatment_pop_mean,
                            treatment_pop_sd) {
  ### generate random sample for control with pop mean and sd inputs
  control <- rnorm(group_sample_size, 
                   mean = control_pop_mean, 
                   sd = control_pop_sd)
  ### generate random sample for treatment with pop mean and sd inputs
  treatment <- rnorm(group_sample_size, 
                     mean = treatment_pop_mean, 
                     sd = treatment_pop_sd)
  t.test(control, treatment)$p.value
}

### 
sim_results <- replicate(10000, welch_ttest_sim(n, c_mu, c_s, t_mu, t_s)) 

### empirical power: proportion of simulation pvalues below 0.05
sum(sim_results < 0.05)/length(sim_results)
```
### Prove simulation ties with power function

```{r}
### pooled sd simple formula: sqrt((s1^2 _ s2^2)/2)
### ties with Cohens formula: sqrt(((n1-1)s1^2 +  (n2-1)s2^2) / (n1+n2-2))
pooled_sd <- sqrt((c_s^2 + t_s^2)/2)

diff_between_means <- c_mu-t_mu
  
effect_size <- diff_between_means/pooled_sd

pwr.t.test(n = 50, d = effect_size, sig.level = 0.05,
           type="two.sample",alternative="two.sided") 
```



### Formula for target sample size
- 


```{r}
library(tidyverse)
library(pwr)
```
### Using pwr package vs power... base functions?
- returns same result

```{r}
?power.t.test

pwr.t.test(n = NULL, 
           d = 0.05/0.25, 
           sig.level = 0.05, 
           power = 0.8, 
           type = 'two.sample',
           alternative = "two.sided")

power.t.test(delta = 0.05, sd = 0.25, 
                   sig.level = 0.05, power = 0.8, 
                   alternative = "two.sided")
```

pwr package vs base R

using cohens D

```{r}
cyl_4 <- mpg %>% filter(cyl==4)
cyl_6 <- mpg %>% filter(cyl==6)


t.test(cyl_4$hwy, cyl_6$hwy)
```


```{r}
ttest_target_sample_size_fun <- function(delta_var, sd_var) {
      res <- power.t.test(delta = delta_var, sd = sd_var, 
                   sig.level = 0.05, power = 0.8, 
                   type = "two.sample")$n
      ceiling(res)
}

ttest_target_sample_size_fun(delta_var = 0.75, sd_var = 2.25)

expand_grid(delta_input = seq(1, 5, 0.5),
            sd_input = seq(0.25, 5, 0.25)) %>%
      rowwise() %>%
      mutate(samples_per_group = ttest_target_sample_size_fun(
                  delta_var =delta_input, sd_var = sd_input
            )
      ) %>%
      ggplot(aes(x=sd_input,
                 y=samples_per_group,
                 group=factor(delta_input),
                 color=factor(delta_input))) +
      geom_point() +
      geom_line()
```

### Related topics / further research
- Variance reduction techniques which can help speed up A/B tests and/or increase statistical power for a test
- 



